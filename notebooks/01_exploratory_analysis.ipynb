{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Discriminator - Exploratory Analysis\n",
    "\n",
    "This notebook demonstrates the usage of the Blood Discriminator system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from preprocessing.data_loader import BloodDataPreprocessor, split_data\n",
    "from models.classifier import CNNBloodClassifier, TraditionalMLClassifier\n",
    "from evaluation.metrics import ModelEvaluator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = BloodDataPreprocessor(img_size=(224, 224), normalize=True)\n",
    "\n",
    "# For demonstration, create synthetic data\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, 224, 224, 3).astype(np.float32)\n",
    "y = np.random.randint(0, 4, n_samples)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    X, y, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['A', 'B', 'AB', 'O']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, data, title in zip(axes, [y_train, y_val, y_test], ['Train', 'Validation', 'Test']):\n",
    "    unique, counts = np.unique(data, return_counts=True)\n",
    "    ax.bar([class_names[i] for i in unique], counts)\n",
    "    ax.set_title(f'{title} Set Distribution')\n",
    "    ax.set_xlabel('Blood Type')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN model\n",
    "cnn_model = CNNBloodClassifier(\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = cnn_model.train(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    epochs=10,  # Reduced for demo\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(history.history['loss'], label='Train')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = cnn_model.predict(X_test)\n",
    "y_pred_proba = cnn_model.model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = ModelEvaluator(class_names=class_names)\n",
    "results = evaluator.comprehensive_evaluation(\n",
    "    y_test, y_pred, y_pred_proba\n",
    ")\n",
    "\n",
    "# Print report\n",
    "report = evaluator.generate_report()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - CNN Model')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with Traditional ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = TraditionalMLClassifier(model_type='random_forest', n_estimators=50)\n",
    "X_train_flat = np.concatenate([X_train, X_val])\n",
    "y_train_flat = np.concatenate([y_train, y_val])\n",
    "rf_model.train(X_train_flat, y_train_flat)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "evaluator_rf = ModelEvaluator(class_names=class_names)\n",
    "results_rf = evaluator_rf.comprehensive_evaluation(y_test, y_pred_rf)\n",
    "\n",
    "# Compare accuracies\n",
    "print(f\"CNN Accuracy: {results['quality']['accuracy']:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {results_rf['quality']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-class metrics\n",
    "metrics_to_plot = ['precision', 'recall', 'f1']\n",
    "cnn_scores = {}\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    cnn_scores[metric] = [results['quality'].get(f'{metric}_{cls}', 0) for cls in class_names]\n",
    "\n",
    "# Plot\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax.bar(x + i*width, cnn_scores[metric], width, label=metric.capitalize())\n",
    "\n",
    "ax.set_xlabel('Blood Type')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Per-Class Performance Metrics - CNN Model')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
