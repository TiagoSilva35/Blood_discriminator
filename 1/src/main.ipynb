{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from utils import *\n",
    "from train import fit, evaluate\n",
    "import CNN, MLP\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We work on the 2D dataset with size 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'bloodmnist'\n",
    "download = True\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we read the MedMNIST data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "validation_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
    "\n",
    "write_log('train_dataset_log.txt', str(train_dataset))\n",
    "write_log('test_dataset_log.txt', str(test_dataset))\n",
    "write_log('validation_dataset_log.txt', str(validation_dataset))\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "validation_loader = data.DataLoader(dataset=validation_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables declaration\n",
    "model_type = \"MLNN\"\n",
    "n_epochs = 15\n",
    "n_layers = 1\n",
    "n_inputs = n_channels * 28 * 28\n",
    "size_hidden_layer = (n_inputs + n_classes) // 2  # n_classes = n_outputs\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = get_device()\n",
    "write_log(\"main_log.txt\", f\"Using device: {device}\\n\")\n",
    "\n",
    "if model_type == \"CNN\":\n",
    "    # create model\n",
    "    model = CNN.CNN(n_channels, n_classes)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    X_train, y_train = process_data(train_loader, flag=False)\n",
    "    X_test, y_test = process_data(test_loader, flag=False)\n",
    "\n",
    "    # train and evaluate per epoch\n",
    "    training_losses = []\n",
    "    training_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    test_f1s = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # train one epoch\n",
    "        epoch_losses, epoch_accs, model = fit(\n",
    "            device,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            n_epochs=1,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "        training_losses.extend(epoch_losses)\n",
    "        training_accs.extend(epoch_accs)\n",
    "\n",
    "        # evaluate on test set\n",
    "        CM_test, f1_test, loss_test, acc_test = evaluate(\n",
    "            device, X_test, y_test, model, loss_function\n",
    "        )\n",
    "        test_losses.append(loss_test)\n",
    "        test_accs.append(acc_test)\n",
    "        test_f1s.append(f1_test)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{n_epochs}] - Test Loss: {loss_test:.4f}, Test Acc: {acc_test:.2f}%, F1: {f1_test:.4f}\"\n",
    "        )\n",
    "\n",
    "    trained_model = model\n",
    "\n",
    "    # final confusion matrix and metrics\n",
    "    CM, f1, final_test_loss, final_test_acc = evaluate(\n",
    "        device, X_test, y_test, trained_model, loss_function\n",
    "    )\n",
    "\n",
    "    precision, sensitivity, specificity = get_metrics(CM)\n",
    "    complexity = sum(p.numel() for p in trained_model.parameters() if p.requires_grad)\n",
    "\n",
    "    # calculate macro averages\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_sensitivity = np.mean(sensitivity)\n",
    "    macro_specificity = np.mean(specificity)\n",
    "\n",
    "    # save results\n",
    "    write_log(\n",
    "        \"training_history.txt\",\n",
    "        f\"Training Losses: {training_losses}\\n\"\n",
    "        f\"Training Accuracies: {training_accs}\\n\"\n",
    "        f\"Test Losses: {test_losses}\\n\"\n",
    "        f\"Test Accuracies: {test_accs}\\n\"\n",
    "        f\"Test F1 Scores: {test_f1s}\\n\",\n",
    "    )\n",
    "\n",
    "    write_log(\n",
    "        \"final_results.txt\",\n",
    "        f\"Final Confusion Matrix:\\n{CM}\\n\"\n",
    "        f\"F1 Score: {f1:.4f}\\n\"\n",
    "        f\"Test Loss: {final_test_loss:.4f}\\n\"\n",
    "        f\"Test Accuracy: {final_test_acc:.2f}%\\n\"\n",
    "        f\"Complexity: {complexity:,} parameters\\n\"\n",
    "        f\"Macro Precision: {macro_precision:.4f}\\n\"\n",
    "        f\"Macro Sensitivity: {macro_sensitivity:.4f}\\n\"\n",
    "        f\"Macro Specificity: {macro_specificity:.4f}\\n\",\n",
    "    )\n",
    "\n",
    "    # plot comparisons\n",
    "    plot_loss(training_losses, test_losses)\n",
    "    plot_acc(training_accs, test_accs)\n",
    "    plot_confusion_matrix(CM, n_classes)\n",
    "    plot_precision_recall(precision, sensitivity, n_classes)\n",
    "\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Test Loss: {final_test_loss:.4f}, Test Acc: {final_test_acc:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Complexity: {complexity:,} parameters\")\n",
    "    print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro Sensitivity: {macro_sensitivity:.4f}\")\n",
    "    print(f\"Macro Specificity: {macro_specificity:.4f}\")\n",
    "\n",
    "\n",
    "elif model_type == \"MLNN\":\n",
    "    model = MLP.MLP(n_inputs, [size_hidden_layer] * n_layers, n_classes, 0.2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # process data\n",
    "    X_train, y_train = process_data(train_loader)\n",
    "    X_test, y_test = process_data(test_loader)\n",
    "\n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    write_log(\n",
    "        \"main_log.txt\",\n",
    "        f\"Processed data shapes: Train: {X_train.shape}, Test: {X_test.shape}\\n\",\n",
    "    )\n",
    "\n",
    "    # train and evaluate per epoch\n",
    "    training_losses = []\n",
    "    training_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    test_f1s = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # train one epoch\n",
    "        epoch_losses, epoch_accs, model = fit(\n",
    "            device,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            n_epochs=1,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "        training_losses.extend(epoch_losses)\n",
    "        training_accs.extend(epoch_accs)\n",
    "\n",
    "        # evaluate on test set\n",
    "        CM_test, f1_test, loss_test, acc_test = evaluate(\n",
    "            device, X_test, y_test, model, loss_function\n",
    "        )\n",
    "        test_losses.append(loss_test)\n",
    "        test_accs.append(acc_test)\n",
    "        test_f1s.append(f1_test)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{n_epochs}] - Test Loss: {loss_test:.4f}, Test Acc: {acc_test:.2f}%, F1: {f1_test:.4f}\"\n",
    "        )\n",
    "\n",
    "    trained_model = model\n",
    "\n",
    "    # final confusion matrix and metrics\n",
    "    CM, f1, final_test_loss, final_test_acc = evaluate(\n",
    "        device, X_test, y_test, trained_model, loss_function\n",
    "    )\n",
    "\n",
    "    precision, sensitivity, specificity = get_metrics(CM)\n",
    "    complexity = sum(p.numel() for p in trained_model.parameters() if p.requires_grad)\n",
    "\n",
    "    # calculate macro averages\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_sensitivity = np.mean(sensitivity)\n",
    "    macro_specificity = np.mean(specificity)\n",
    "\n",
    "    # save results\n",
    "    write_log(\n",
    "        \"training_history.txt\",\n",
    "        f\"Training Losses: {training_losses}\\n\"\n",
    "        f\"Training Accuracies: {training_accs}\\n\"\n",
    "        f\"Test Losses: {test_losses}\\n\"\n",
    "        f\"Test Accuracies: {test_accs}\\n\"\n",
    "        f\"Test F1 Scores: {test_f1s}\\n\",\n",
    "    )\n",
    "\n",
    "    write_log(\n",
    "        \"final_results.txt\",\n",
    "        f\"Final Confusion Matrix:\\n{CM}\\n\"\n",
    "        f\"F1 Score: {f1:.4f}\\n\"\n",
    "        f\"Test Loss: {final_test_loss:.4f}\\n\"\n",
    "        f\"Test Accuracy: {final_test_acc:.2f}%\\n\"\n",
    "        f\"Complexity: {complexity:,} parameters\\n\"\n",
    "        f\"Macro Precision: {macro_precision:.4f}\\n\"\n",
    "        f\"Macro Sensitivity: {macro_sensitivity:.4f}\\n\"\n",
    "        f\"Macro Specificity: {macro_specificity:.4f}\\n\",\n",
    "    )\n",
    "\n",
    "    # plot comparisons\n",
    "    plot_loss(training_losses, test_losses)\n",
    "    plot_acc(training_accs, test_accs)\n",
    "    plot_confusion_matrix(CM, n_classes)\n",
    "    plot_precision_recall(precision, sensitivity, n_classes)\n",
    "\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Test Loss: {final_test_loss:.4f}, Test Acc: {final_test_acc:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Complexity: {complexity:,} parameters\")\n",
    "    print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro Sensitivity: {macro_sensitivity:.4f}\")\n",
    "    print(f\"Macro Specificity: {macro_specificity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blood",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
