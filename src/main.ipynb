{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from utils import write_log, get_device, process_data\n",
    "from train import fit, evaluate\n",
    "import CNN, DNN, MLP\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We work on the 2D dataset with size 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FLAG = \"bloodmnist\"\n",
    "DOWNLOAD = True\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "info = INFO[DATA_FLAG]\n",
    "task = info[\"task\"]\n",
    "N_CHANNELS = info[\"n_channels\"]\n",
    "N_CLASSES = len(info[\"label\"])\n",
    "\n",
    "DataClass = getattr(medmnist, info[\"python_class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we read the MedMNIST data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split=\"train\", transform=data_transform, download=DOWNLOAD)\n",
    "test_dataset = DataClass(split=\"test\", transform=data_transform, download=DOWNLOAD)\n",
    "validation_dataset = DataClass(split=\"val\", transform=data_transform, download=DOWNLOAD)\n",
    "\n",
    "write_log(\"train_dataset_log.txt\", str(train_dataset))\n",
    "write_log(\"test_dataset_log.txt\", str(test_dataset))\n",
    "write_log(\"validation_dataset_log.txt\", str(validation_dataset))\n",
    "\n",
    "pil_dataset = DataClass(split=\"train\", download=DOWNLOAD)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "train_loader_at_eval = data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=2 * BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=2 * BATCH_SIZE, shuffle=False\n",
    ")\n",
    "validation_loader = data.DataLoader(\n",
    "    dataset=validation_dataset, batch_size=2 * BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 148.6486\n",
      "Epoch [2/15], Loss: 91.0126\n",
      "Epoch [3/15], Loss: 71.0893\n",
      "Epoch [4/15], Loss: 63.0121\n",
      "Epoch [5/15], Loss: 56.8830\n",
      "Epoch [6/15], Loss: 53.8477\n",
      "Epoch [7/15], Loss: 50.4935\n",
      "Epoch [8/15], Loss: 47.8459\n",
      "Epoch [9/15], Loss: 45.1568\n",
      "Epoch [10/15], Loss: 41.5441\n",
      "Epoch [11/15], Loss: 38.8114\n",
      "Epoch [12/15], Loss: 35.7265\n",
      "Epoch [13/15], Loss: 32.5733\n",
      "Epoch [14/15], Loss: 31.0399\n",
      "Epoch [15/15], Loss: 28.8438\n"
     ]
    }
   ],
   "source": [
    "# variables declaration\n",
    "MODEL_TYPE = \"CNN\"\n",
    "N_EPOCHS = 15\n",
    "N_LAYERS = 1\n",
    "N_INPUTS = N_CHANNELS * 28 * 28\n",
    "SIZE_HIDDEN_LAYER = (N_INPUTS + N_CLASSES) // 2  # N_CLASSES = n_outputs\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "device = get_device()\n",
    "write_log(\"main_log.txt\", f\"Using device: {device}\\n\")\n",
    "\n",
    "if MODEL_TYPE == \"CNN\":\n",
    "    # model creation\n",
    "    model = CNN.CNN(N_CHANNELS, N_CLASSES)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # process training data\n",
    "    X, y = process_data(train_loader, flag=False)\n",
    "    write_log(\n",
    "        \"main_log.txt\", f\"Processed training data shapes: X: {X.shape}, y: {y.shape}\\n\"\n",
    "    )\n",
    "\n",
    "    # main training loop\n",
    "    loss_values_training, trained_model = fit(\n",
    "        device,\n",
    "        X,\n",
    "        y,\n",
    "        model,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        N_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    write_log(\"loss_values_training.txt\", str(loss_values_training))\n",
    "\n",
    "    # evaluate data\n",
    "    X, y = process_data(validation_loader, flag=False)\n",
    "    CM_val, f1_val = evaluate(device, X, y, trained_model, batch_size=BATCH_SIZE)\n",
    "    write_log(\n",
    "        \"validation_results.txt\", f\"Confusion Matrix:\\n{CM_val}\\nF1 Score: {f1_val}\\n\"\n",
    "    )\n",
    "\n",
    "    # test data\n",
    "    X, y = process_data(test_loader, flag=False)\n",
    "    CM, f1 = evaluate(device, X, y, trained_model, batch_size=BATCH_SIZE)\n",
    "    write_log(\"results.txt\", f\"Confusion Matrix:\\n{CM}\\nF1 Score: {f1}\\n\")\n",
    "\n",
    "elif MODEL_TYPE == \"DNN\":\n",
    "    # model creation\n",
    "    model = DNN.DNN(N_INPUTS, [SIZE_HIDDEN_LAYER] * N_LAYERS, N_CLASSES)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # process training data\n",
    "    X, y = process_data(train_loader)\n",
    "    write_log(\n",
    "        \"main_log.txt\", f\"Processed training data shapes: X: {X.shape}, y: {y.shape}\\n\"\n",
    "    )\n",
    "\n",
    "    # main training loop\n",
    "    loss_values_training, trained_model = fit(\n",
    "        device,\n",
    "        X,\n",
    "        y,\n",
    "        model,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        N_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    write_log(\"loss_values_training.txt\", str(loss_values_training))\n",
    "\n",
    "    # evaluate data\n",
    "    X, y = process_data(validation_loader)\n",
    "    CM_val, f1_val = evaluate(device, X, y, trained_model, batch_size=BATCH_SIZE)\n",
    "    write_log(\n",
    "        \"validation_results.txt\", f\"Confusion Matrix:\\n{CM_val}\\nF1 Score: {f1_val}\\n\"\n",
    "    )\n",
    "\n",
    "    # test data\n",
    "    X, y = process_data(test_loader)\n",
    "    CM, f1 = evaluate(device, X, y, trained_model, batch_size=BATCH_SIZE)\n",
    "    write_log(\"results.txt\", f\"Confusion Matrix:\\n{CM}\\nF1 Score: {f1}\\n\")\n",
    "\n",
    "elif MODEL_TYPE == \"MLP\":\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blood",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
