{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from utils import write_log, get_device, process_data\n",
    "from train import fit, evaluate\n",
    "import CNN, DNN, MLP\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We work on the 2D dataset with size 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'bloodmnist'\n",
    "download = True\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we read the MedMNIST data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "validation_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
    "\n",
    "write_log('train_dataset_log.txt', str(train_dataset))\n",
    "write_log('test_dataset_log.txt', str(test_dataset))\n",
    "write_log('validation_dataset_log.txt', str(validation_dataset))\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "validation_loader = data.DataLoader(dataset=validation_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 139.2199\n",
      "Epoch [2/20], Loss: 81.1620\n",
      "Epoch [3/20], Loss: 71.6596\n",
      "Epoch [4/20], Loss: 62.1725\n",
      "Epoch [5/20], Loss: 56.1907\n",
      "Epoch [6/20], Loss: 50.6283\n",
      "Epoch [7/20], Loss: 47.3393\n",
      "Epoch [8/20], Loss: 43.9701\n",
      "Epoch [9/20], Loss: 41.8174\n",
      "Epoch [10/20], Loss: 39.8113\n",
      "Epoch [11/20], Loss: 36.4410\n",
      "Epoch [12/20], Loss: 33.6434\n",
      "Epoch [13/20], Loss: 31.5893\n",
      "Epoch [14/20], Loss: 29.4098\n",
      "Epoch [15/20], Loss: 28.2920\n",
      "Epoch [16/20], Loss: 26.1648\n",
      "Epoch [17/20], Loss: 24.4867\n",
      "Epoch [18/20], Loss: 23.4108\n",
      "Epoch [19/20], Loss: 21.9860\n",
      "Epoch [20/20], Loss: 20.8307\n"
     ]
    }
   ],
   "source": [
    "# variables declaration\n",
    "model_type = 'CNN'\n",
    "n_epochs = 20\n",
    "n_layers = 1\n",
    "n_inputs = n_channels * 28 * 28\n",
    "size_hidden_layer = (n_inputs + n_classes) // 2 # n_classes = n_outputs\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "assert device is not None\n",
    "\n",
    "write_log('main_log.txt', f'Using device: {device}\\n')\n",
    "\n",
    "assert model_type is not None\n",
    "\n",
    "if model_type == 'CNN':\n",
    "    model = CNN.CNN(n_channels, n_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    X, y = process_data(train_loader, flag=False)\n",
    "    loss_values_training, trained_model = fit(device, X, y, model, loss_function, optimizer, n_epochs, batch_size=BATCH_SIZE)\n",
    "    write_log('loss_values_training.txt', str(loss_values_training))\n",
    "\n",
    "    # evaluate data\n",
    "    X, y = process_data(validation_loader, flag=False)\n",
    "    CM_val, f1_val = evaluate(device, X, y, trained_model, batch_size=BATCH_SIZE)\n",
    "    write_log('validation_results.txt', f'Confusion Matrix:\\n{CM_val}\\nF1 Score: {f1_val}\\n')\n",
    "\n",
    "\n",
    "    # test data\n",
    "    X, y = process_data(test_loader, flag=False)\n",
    "    CM, f1 = evaluate(device, X, y, trained_model, batch_size=BATCH_SIZE)\n",
    "    write_log('results.txt', f'Confusion Matrix:\\n{CM}\\nF1 Score: {f1}\\n')\n",
    "\n",
    "\n",
    "\n",
    "elif model_type == 'MLNN':\n",
    "    model = DNN.DNN(n_inputs, [size_hidden_layer] * n_layers, n_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # train loader is an array of tuples (X, y) where X is a batch of images and y are the corresponding labels\n",
    "    # extract X and y\n",
    "    # train data    \n",
    "    X, y = process_data(train_loader)\n",
    "    print(X.shape, y.shape) \n",
    "    write_log(\"main_log.txt\", f\"Processed training data shapes: X: {X.shape}, y: {y.shape}\\n\")\n",
    "    loss_values_training, trained_model = fit(device, X, y, model, loss_function, optimizer, n_epochs, batch_size=BATCH_SIZE)\n",
    "    write_log('loss_values_training.txt', str(loss_values_training))\n",
    "    \n",
    "    # evaluate data\n",
    "    X, y = process_data(validation_loader)\n",
    "    CM_val, f1_val = evaluate(device, X, y, trained_model, batch_size=BATCH_SIZE)\n",
    "    write_log('validation_results.txt', f'Confusion Matrix:\\n{CM_val}\\nF1 Score: {f1_val}\\n')\n",
    "\n",
    "\n",
    "\n",
    "    # test data\n",
    "    X, y = process_data(test_loader) \n",
    "    CM, f1 = evaluate(device, X, y, trained_model, batch_size=BATCH_SIZE)\n",
    "    write_log('results.txt', f'Confusion Matrix:\\n{CM}\\nF1 Score: {f1}\\n')\n",
    "\n",
    "\n",
    "else:\n",
    "    # model = MLP.MLP()\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
