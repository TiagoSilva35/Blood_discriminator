{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from utils import *\n",
    "from train import fit, evaluate\n",
    "import CNN, MLP\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We work on the 2D dataset with size 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'bloodmnist'\n",
    "download = True\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we read the MedMNIST data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "validation_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
    "\n",
    "write_log('train_dataset_log.txt', str(train_dataset))\n",
    "write_log('test_dataset_log.txt', str(test_dataset))\n",
    "write_log('validation_dataset_log.txt', str(validation_dataset))\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "validation_loader = data.DataLoader(dataset=validation_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     38\u001b[39m test_f1s = []\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# train one epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     epoch_losses, epoch_accs, model = \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     training_losses.extend(epoch_losses)\n\u001b[32m     53\u001b[39m     training_accs.extend(epoch_accs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\git\\uni\\Blood_discriminator\\src\\train.py:51\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(device, X_train, y_train, nn, criterion, optimizer, n_epochs, to_device, batch_size)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[32m     49\u001b[39m outputs = nn(X_batch)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(criterion, torch.nn.MSELoss):\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Convert targets to one-hot\u001b[39;00m\n\u001b[32m     53\u001b[39m     y_batch_onehot = torch.zeros(y_batch.size(\u001b[32m0\u001b[39m), outputs.size(\u001b[32m1\u001b[39m)).to(device)\n\u001b[32m     54\u001b[39m     y_batch_onehot.scatter_(\u001b[32m1\u001b[39m, y_batch.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\git\\uni\\Blood_discriminator\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\git\\uni\\Blood_discriminator\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\git\\uni\\Blood_discriminator\\env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:634\u001b[39m, in \u001b[36mMSELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\git\\uni\\Blood_discriminator\\env\\Lib\\site-packages\\torch\\nn\\functional.py:3864\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3861\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3862\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3864\u001b[39m expanded_input, expanded_target = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3866\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3867\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight.size() != \u001b[38;5;28minput\u001b[39m.size():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\git\\uni\\Blood_discriminator\\env\\Lib\\site-packages\\torch\\functional.py:77\u001b[39m, in \u001b[36mbroadcast_tensors\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (8) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# variables declaration\n",
    "model_type = \"CNN\"\n",
    "n_epochs = 15\n",
    "n_layers = 1\n",
    "n_inputs = n_channels * 28 * 28\n",
    "size_hidden_layer = (n_inputs + n_classes) // 2  # n_classes = n_outputs\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# loss_function = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "assert device is not None\n",
    "\n",
    "write_log(\"main_log.txt\", f\"Using device: {device}\\n\")\n",
    "\n",
    "assert model_type is not None\n",
    "\n",
    "if model_type == \"CNN\":\n",
    "    # create model\n",
    "    model = CNN.CNN(n_channels, n_classes)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    X_train, y_train = process_data(train_loader, flag=False)\n",
    "    X_test, y_test = process_data(test_loader, flag=False)\n",
    "\n",
    "    # train and evaluate per epoch\n",
    "    training_losses = []\n",
    "    training_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    test_f1s = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # train one epoch\n",
    "        epoch_losses, epoch_accs, model = fit(\n",
    "            device,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            n_epochs=1,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "        training_losses.extend(epoch_losses)\n",
    "        training_accs.extend(epoch_accs)\n",
    "\n",
    "        # evaluate on test set\n",
    "        CM_test, f1_test, loss_test, acc_test = evaluate(\n",
    "            device, X_test, y_test, model, loss_function\n",
    "        )\n",
    "        test_losses.append(loss_test)\n",
    "        test_accs.append(acc_test)\n",
    "        test_f1s.append(f1_test)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{n_epochs}] - Test Loss: {loss_test:.4f}, Test Acc: {acc_test:.2f}%, F1: {f1_test:.4f}\"\n",
    "        )\n",
    "\n",
    "    trained_model = model\n",
    "\n",
    "    # final confusion matrix and metrics\n",
    "    CM, f1, final_test_loss, final_test_acc = evaluate(\n",
    "        device, X_test, y_test, trained_model, loss_function\n",
    "    )\n",
    "\n",
    "    precision, sensitivity, specificity = get_metrics(CM)\n",
    "    complexity = sum(p.numel() for p in trained_model.parameters() if p.requires_grad)\n",
    "\n",
    "    # calculate macro averages\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_sensitivity = np.mean(sensitivity)\n",
    "    macro_specificity = np.mean(specificity)\n",
    "\n",
    "    # save results\n",
    "    write_log(\n",
    "        \"training_history.txt\",\n",
    "        f\"Training Losses: {training_losses}\\n\"\n",
    "        f\"Training Accuracies: {training_accs}\\n\"\n",
    "        f\"Test Losses: {test_losses}\\n\"\n",
    "        f\"Test Accuracies: {test_accs}\\n\"\n",
    "        f\"Test F1 Scores: {test_f1s}\\n\",\n",
    "    )\n",
    "\n",
    "    write_log(\n",
    "        \"final_results.txt\",\n",
    "        f\"Final Confusion Matrix:\\n{CM}\\n\"\n",
    "        f\"F1 Score: {f1:.4f}\\n\"\n",
    "        f\"Test Loss: {final_test_loss:.4f}\\n\"\n",
    "        f\"Test Accuracy: {final_test_acc:.2f}%\\n\"\n",
    "        f\"Complexity: {complexity:,} parameters\\n\"\n",
    "        f\"Macro Precision: {macro_precision:.4f}\\n\"\n",
    "        f\"Macro Sensitivity: {macro_sensitivity:.4f}\\n\"\n",
    "        f\"Macro Specificity: {macro_specificity:.4f}\\n\",\n",
    "    )\n",
    "\n",
    "    # Plot comparisons\n",
    "    plot_loss(training_losses, test_losses)\n",
    "    plot_acc(training_accs, test_accs)\n",
    "    plot_confusion_matrix(CM, n_classes)\n",
    "    plot_precision_recall(precision, sensitivity, n_classes)\n",
    "\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Test Loss: {final_test_loss:.4f}, Test Acc: {final_test_acc:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Complexity: {complexity:,} parameters\")\n",
    "    print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro Sensitivity: {macro_sensitivity:.4f}\")\n",
    "    print(f\"Macro Specificity: {macro_specificity:.4f}\")\n",
    "\n",
    "\n",
    "elif model_type == \"MLNN\":\n",
    "    model = MLP.MLP(n_inputs, [size_hidden_layer] * n_layers, n_classes, 0.2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # process data\n",
    "    X_train, y_train = process_data(train_loader)\n",
    "    X_test, y_test = process_data(test_loader)\n",
    "\n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    write_log(\n",
    "        \"main_log.txt\",\n",
    "        f\"Processed data shapes: Train: {X_train.shape}, Test: {X_test.shape}\\n\",\n",
    "    )\n",
    "\n",
    "    # train and evaluate per epoch\n",
    "    training_losses = []\n",
    "    training_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    test_f1s = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # train one epoch\n",
    "        epoch_losses, epoch_accs, model = fit(\n",
    "            device,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            n_epochs=1,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "        training_losses.extend(epoch_losses)\n",
    "        training_accs.extend(epoch_accs)\n",
    "\n",
    "        # evaluate on test set\n",
    "        CM_test, f1_test, loss_test, acc_test = evaluate(\n",
    "            device, X_test, y_test, model, loss_function\n",
    "        )\n",
    "        test_losses.append(loss_test)\n",
    "        test_accs.append(acc_test)\n",
    "        test_f1s.append(f1_test)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{n_epochs}] - Test Loss: {loss_test:.4f}, Test Acc: {acc_test:.2f}%, F1: {f1_test:.4f}\"\n",
    "        )\n",
    "\n",
    "    trained_model = model\n",
    "\n",
    "    # final confusion matrix and metrics\n",
    "    CM, f1, final_test_loss, final_test_acc = evaluate(\n",
    "        device, X_test, y_test, trained_model, loss_function\n",
    "    )\n",
    "\n",
    "    precision, sensitivity, specificity = get_metrics(CM)\n",
    "    complexity = sum(p.numel() for p in trained_model.parameters() if p.requires_grad)\n",
    "\n",
    "    # calculate macro averages\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_sensitivity = np.mean(sensitivity)\n",
    "    macro_specificity = np.mean(specificity)\n",
    "\n",
    "    # save results\n",
    "    write_log(\n",
    "        \"training_history.txt\",\n",
    "        f\"Training Losses: {training_losses}\\n\"\n",
    "        f\"Training Accuracies: {training_accs}\\n\"\n",
    "        f\"Test Losses: {test_losses}\\n\"\n",
    "        f\"Test Accuracies: {test_accs}\\n\"\n",
    "        f\"Test F1 Scores: {test_f1s}\\n\",\n",
    "    )\n",
    "\n",
    "    write_log(\n",
    "        \"final_results.txt\",\n",
    "        f\"Final Confusion Matrix:\\n{CM}\\n\"\n",
    "        f\"F1 Score: {f1:.4f}\\n\"\n",
    "        f\"Test Loss: {final_test_loss:.4f}\\n\"\n",
    "        f\"Test Accuracy: {final_test_acc:.2f}%\\n\"\n",
    "        f\"Complexity: {complexity:,} parameters\\n\"\n",
    "        f\"Macro Precision: {macro_precision:.4f}\\n\"\n",
    "        f\"Macro Sensitivity: {macro_sensitivity:.4f}\\n\"\n",
    "        f\"Macro Specificity: {macro_specificity:.4f}\\n\",\n",
    "    )\n",
    "\n",
    "    # plot comparisons\n",
    "    plot_loss(training_losses, test_losses)\n",
    "    plot_acc(training_accs, test_accs)\n",
    "    plot_confusion_matrix(CM, n_classes)\n",
    "    plot_precision_recall(precision, sensitivity, n_classes)\n",
    "\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Test Loss: {final_test_loss:.4f}, Test Acc: {final_test_acc:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Complexity: {complexity:,} parameters\")\n",
    "    print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro Sensitivity: {macro_sensitivity:.4f}\")\n",
    "    print(f\"Macro Specificity: {macro_specificity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blood",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
