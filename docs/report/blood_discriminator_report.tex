\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

\begin{document}

\title{Blood Cell Classification Using Deep Learning: MLNN and CNN Architectures}
\titlerunning{Blood Cell Classification Using Deep Learning}
\author{Miguel Cabral Pinto \& Tiago Silva}
\authorrunning{Miguel Cabral Pinto \& Tiago Silva}

\institute{
    University of Coimbra\\
}

\maketitle

\begin{abstract}
    Blood cell classification is a critical task in medical diagnosis and hematological analysis.
    The present work conducts a comparative study between two deep learning architectures for blood cell classification supported by the BloodMNIST dataset.
    We implement and evaluate two neural network architectures: a Convolutional Neural Network (CNN) and a Multi-Layer Neural Network (MLNN).
    Besides implementation details we created an evaluation framework composed by three dimensions: quality (accuracy, precision, recall, F1-score), efficacy (sensitivity, specificity), and efficiency (training time, inference time).
    Experimental results support the theoretical superiority of CNN architectures for image-based classification tasks, showing significantly better performance than traditional fully-connected networks although the imbalanced nature of the dataset poses challenges for both architectures.

    \keywords{Blood Cell Classification \and Deep Learning \and Multi-Layer Neural Networks \and Convolutional Neural Networks \and Medical Imaging \and BloodMNIST}
\end{abstract}

\section{Methodology}

\subsection{Dataset}
We utilize the BloodMNIST dataset from the MedMNIST collection. The dataset
characteristics are:

\begin{itemize}
    \item \textbf{Image Size}: 28$\times$28 pixels, RGB (3 channels)
    \item \textbf{Classes}: 8 blood cell types
    \item \textbf{Training Set}: 11,959 images
    \item \textbf{Validation Set}: 1,712 images
    \item \textbf{Test Set}: 3,421 images
    \item \textbf{Total}: 17,092 images
\end{itemize}

\noindent
Because the dataset is already preprocessed and normalized, we focus on the model architectures and training procedures rather than data augmentation techniques.
As expected in medical datasets, class imbalance exists, with some cell types being underrepresented.
This is addressed in the evaluation phase using metrics sensitive to class distribution
to ensure a more robust assessment of model performance.

\subsection{Model Architectures}

\subsubsection{Convolutional Neural Network (CNN)}
Our CNN architecture consists of three convolutional blocks followed by
fully-connected layers. The design follows the principle of progressively
increasing feature channels while reducing spatial dimensions:

\begin{algorithm}
    \caption{CNN Architecture}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} $x \in \mathbb{R}^{B \times 3 \times 28 \times 28}$
        \STATE \textbf{Conv\textsubscript{1}:} Conv2d(3 $\rightarrow$ 32, kernel=3, padding=1) + ReLU + MaxPool(2)
        \STATE \quad Output: $\mathbb{R}^{B \times 32 \times 14 \times 14}$
        \STATE \textbf{Conv\textsubscript{2}:} Conv2d(32 $\rightarrow$ 64, kernel=3, padding=1) + ReLU + MaxPool(2)
        \STATE \quad Output: $\mathbb{R}^{B \times 64 \times 7 \times 7}$
        \STATE \textbf{Conv\textsubscript{3}:} Conv2d(64 $\rightarrow$ 128, kernel=3, padding=1) + ReLU + MaxPool(2)
        \STATE \quad Output: $\mathbb{R}^{B \times 128 \times 3 \times 3}$
        \STATE \textbf{Flatten:} $\mathbb{R}^{B \times 1152}$
        \STATE \textbf{FC\textsubscript{1}:} Linear(1152 $\rightarrow$ 256) + ReLU
        \STATE \textbf{FC\textsubscript{2}:} Linear(256 $\rightarrow$ 128) + ReLU
        \STATE \textbf{FC\textsubscript{3}:} Linear(128 $\rightarrow$ 8)
        \STATE \textbf{Output:} Logits $\in \mathbb{R}^{B \times 8}$
    \end{algorithmic}
\end{algorithm}

\noindent
The architecture has 422,344 total parameters, and its rationale is as follows:

\begin{itemize}
    \item \textbf{Progressive channel expansion} (32$\rightarrow$64$\rightarrow$128): Captures increasingly complex features
    \item \textbf{Spatial dimension reduction} (28$\rightarrow$14$\rightarrow$7$\rightarrow$3): Achieved through max-pooling with stride 2
    \item \textbf{Unitary padding}: Preserves spatial dimensions within each convolutional layer
    \item \textbf{ReLU activation}: Introduces non-linearity and mitigates vanishing gradients
    \item \textbf{Multi-stage FC layers}: Provides smooth transition from feature maps to class predictions
\end{itemize}

\noindent
The number of convolutional layers resulted from balancing model complexity and computational efficiency.
Given the input, we recognized that three convolutional layers would provide enough feature extraction while not over-reducing the spatial dimensions.
The channel progression and kernel sizes were selected based on established (state-of-the-art) CNN design principles proven and discussed in theoretical classes.
Additionally, the fully-connected layers were sized to gradually reduce the feature representation to the final class logits (preventing abrupt bottlenecks).
Though the design could certainly be further optimized through hyperparameter tuning, this architecture serves as a solid baseline (experimentally validated in the results section).

\subsubsection{Multi-Layer Neural Network (MLNN)}
The MLNN architecture uses only fully-connected layers:

\begin{algorithm}
    \caption{MLNN Architecture}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} $x \in \mathbb{R}^{B \times 3 \times 28 \times 28}$
        \STATE \textbf{Flatten:} $x \in \mathbb{R}^{B \times 2352}$ (where $2352 = 3 \times 28 \times 28$)
        \STATE \textbf{FC\textsubscript{1}:} Linear(2352 $\rightarrow$ $h_1$) + ReLU
        \STATE \textbf{FC\textsubscript{2}-FC\textsubscript{n}:} Linear($h_{i-1}$ $\rightarrow$ $h_i$) + ReLU (for $n$ hidden layers)
        \STATE \textbf{Output Layer:} Linear($h_n$ $\rightarrow$ 8)
        \STATE \textbf{Output:} Logits $\in \mathbb{R}^{B \times 8}$
    \end{algorithmic}
\end{algorithm}

\noindent
The size of the hidden layers in the network is calculated using the average of the input and output dimensions:
\begin{equation}
    h = \frac{n_{input} + n_{classes}}{2} = \frac{2352 + 8}{2} = 1180
\end{equation}
This heuristic provides reasonable capacity without excessive parameters.
For a single hidden layer configuration, it results in 1,180 hidden units, creating a bottleneck architecture (2352 → 1180 → 8) that forces the network to learn compressed representations.
While this approach can work for simple classification tasks, it fundamentally treats each pixel as an independent feature, ignoring the 2D spatial relationships that characterize cell morphology. \\
\noindent
The MLNN architecture serves primarily as a pedagogical baseline to demonstrate why CNNs are preferred for image classification.
The lack of translation invariance, the enormous first-layer parameter count (2.77M parameters just in the first layer), and the immediate loss of spatial structure highlight the architectural disadvantages of this approach when compared to convolutional ones.
As such, this work validates theoretical understanding through empirical evidence.

\subsection{Training Configuration: MLNN}

\begin{table}[htbp]
    %\vspace{-0.5cm}
    \centering
    \caption{Experimental Hyperparameters}
    \label{tab:hyperparametersMLNN}
    \begin{tabular}{lll}
        \toprule
        \textbf{Parameter}  & \textbf{MLNN-Test1} & \textbf{MLNN-Test2}   \\
        \midrule
        Learning Rate       & 0.001               & 0.01                  \\
        Optimizer           & Adam                & SGD w/ Momentum (0.9) \\
        Loss Function       & CrossEntropyLoss    & CrossEntropyLoss      \\
        Epochs              & 15                  & 15                    \\
        Hidden Layers       & 1                   & 1                     \\
        Hidden Size         & 1180                & 1180                  \\
        Activation Function & ReLU                & ReLU                  \\
        \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tab:hyperparametersMLNN} presents the complete hyperparameter
configuration used in our experiments, chosen to both experiment with different
training dynamics and provide a solid baseline for comparison with the CNN
architecture.

Here are the main reasons behind our choices:
\begin{itemize}
    \item CrossEntropyLoss was chosen as the standard loss function for multi-class
          classification, combining LogSoftmax and NLLLoss for numerical stability. This
          is particularly important given the class imbalanced nature of the dataset,
          CrossEntropyLoss poses a powerful approach to handle such scenarios by
          adjusting class influence through customized weighting if necessary.
    \item The state of the art Adam optimizer handles the varying gradient magnitudes
          across different network depths was selected due to its innovative adaptive
          learning rate mechanism, combining the benefits of RMSProp and momentum-based
          SGD. To compare with the forementioned momentum-based SGD optimizer, we also
          implemented the same MLNN using this optimizer (with momentum set to 0.9) and a
          higher learning rate (0.01 because in contrast to Adam, SGD lacks adaptive
          learning rates, therefore, a higher initial learning rate is necessary to
          achieve comparable performance).
    \item The 0.001 learning rate represents the standard default for Adam in computer
          vision tasks. Preliminary experiments (not detailed here) validated this
          choice: higher rates (0.01) caused training instability, while lower rates
          (0.0001) converged too slowly. The selected rate achieves a smooth loss
          decrease (1.58 → 0.313) without oscillations, with test performance plateauing
          around epoch 12, indicating appropriate convergence behavior.
    \item The 15-epoch training duration was determined empirically.
    \item He initialization was used throughout, as it is specifically designed for ReLU
          activations and prevents vanishing/exploding gradients by maintaining variance
          across layers.
\end{itemize}

\newpage
\section{Results}
In this section we first present and analyze the results obtained with each type of 
deep learning architecture: MLNN and CNN. Then we proceed to a comparative analysis between both.
\subsection{MLNN}
Two MLNN configurations were tested, differing in optimizer and learning rate
(as detailed in Table~\ref{tab:hyperparametersMLNN}). Let's analyze the results
and make some observations.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlnn_test1_loss.png}
        \caption{(Adam, lr=0.001)}
        \label{fig:mlnn_test1_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlnn_test2_loss.png}
        \caption{(SGD+Momentum, lr=0.01)}
        \label{fig:mlnn_test2_loss}
    \end{subfigure}
    \caption{Loss curves comparison between MLNN configurations}
    \label{fig:mlnn_loss_comparison}
\end{figure}

Looking at the loss curves in Figure~\ref{fig:mlnn_loss_comparison}, we can see
that both configurations show a decreasing trend, indicating that the models
are learning. However, the Adam optimizer (Figure~\ref{fig:mlnn_test1_loss})
demonstrates a more stable and consistent decrease in loss compared to the SGD
with Momentum (Figure~\ref{fig:mlnn_test2_loss}), which exhibits a more erratic
pattern, oscillating significantly during training. This suggests that Adam is
better suited because not only does it adapt the learning rate for each
parameter, but it also combines the benefits of both momentum and RMSProp,
leading to more efficient convergence. This is a relatively simple task, hence
both optimizers manage to reduce the loss. If this were a more complex task,
the differences would be more pronounced.

\newpage
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlnn_test1_accuracy.png}
        \caption{(Adam, lr=0.001)}
        \label{fig:mlnn_test1_acc}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlnn_test2_accuracy.png}
        \caption{(SGD+Momentum, lr=0.01)}
        \label{fig:mlnn_test2_acc}
    \end{subfigure}
    \caption{Accuracy curves comparison between MLNN configurations}
    \label{fig:mlnn_acc_comparison}
\end{figure}

Similarly, the accuracy curves in Figure~\ref{fig:mlnn_acc_comparison} show
that the Adam optimizer achieves higher accuracy more quickly than SGD with
Momentum. The Adam configuration reaches approximately 85\% accuracy by epoch
10, while the SGD configuration lags behind, only reaching around 75\% accuracy
by the same epoch. This further supports the conclusion that Adam's adaptive
learning rates and momentum mechanisms provide a significant advantage in
training efficiency.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlnn_test1_sens_recall.png}
        \caption{MLNN-Test1}
        \label{fig:mlnn_test1_pr}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlnn_test2_sens_recall.png}
        \caption{MLNN-Test2}
        \label{fig:mlnn_test2_pr}
    \end{subfigure}
    \caption{Precision and Recall comparison across classes}
    \label{fig:mlnn_pr_comparison}
\end{figure}

The bar charts in Figure~\ref{fig:mlnn_pr_comparison} illustrate the precision
and recall for each class in both MLNN configurations. We observe that the Adam
optimizer (Figure~\ref{fig:mlnn_test1_pr}) generally outperforms the SGD with
Momentum (Figure~\ref{fig:mlnn_test2_pr}) across most classes. Notably, classes
with fewer samples (such as class 0) exhibit lower precision and recall in both
configurations, highlighting the challenges posed by class imbalance. Despite
in most classes the recall and precision values are relatively high, indicating
that the models thresholds are effective at identifying the majority of
instances correctly, there is still room for improvement, especially for
underrepresented classes.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlnn_test1_CM.png}
        \caption{MLNN-Test1}
        \label{fig:mlnn_test1_cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlnn_test2_CM.png}
        \caption{MLNN-Test2}
        \label{fig:mlnn_test2_cm}
    \end{subfigure}
    \caption{Confusion matrices for both MLNN configurations}
    \label{fig:mlnn_cm_comparison}
\end{figure}

The confusion matrix in Figure~\ref{fig:mlnn_test2_cm} shows that the momentum-based
SGD optimizer is a bit less stable in comparison to Adam because, for exmaple,
it tends to classify more samples as class 3. This could be due to the optimizer's
inability to adapt learning rates for individual parameters, leading to suboptimal
updates. In contrast, the Adam optimizer (Figure~\ref{fig:mlnn_test1_cm}) demonstrates a more balanced
classification across classes, with fewer misclassifications. This further
emphasizes Adam's effectiveness across several metrics.
\newline
\newline
The F1 score, is around 0.80 for both configurations, with Adam slightly outperforming
SGD with Momentum. This indicates that while both models are effective, and for simple tasks
like this one the differences are not very pronounced, Adam provides a marginal advantage in balancing
precision and recall (the thresholds are better optimized).


\subsection{CNN}

\subsection{Comparative Analysis}


\newpage
\begin{thebibliography}{10}

    \bibitem{esteva2019guide}
    Esteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V., DePristo, M., Chou, K., Cui, C., Corrado, G., Thrun, S., Dean, J.:
    A guide to deep learning in healthcare.
    Nature medicine 25(1), 24--29 (2019)

    \bibitem{lecun2015deep}
    LeCun, Y., Bengio, Y., Hinton, G.:
    Deep learning.
    Nature 521(7553), 436--444 (2015)

    \bibitem{krizhevsky2012imagenet}
    Krizhevsky, A., Sutskever, I., Hinton, G.E.:
    Imagenet classification with deep convolutional neural networks.
    Advances in neural information processing systems 25 (2012)

    \bibitem{habibzadeh2011comparative}
    Habibzadeh, M., Krzy{\.z}ak, A., Fevens, T.:
    Comparative study of shape, intensity and texture features and support vector machine for white blood cell classification.
    Journal of Theoretical and Applied Computer Science 5(1), 20--35 (2011)

    \bibitem{acevedo2020recognition}
    Acevedo, A., Merino, A., Alférez, S., Molina, Á., Boldú, L., Rodellar, J.:
    A dataset of microscopic peripheral blood cell images for development of automatic recognition systems.
    Data in brief 30, 105474 (2020)

    \bibitem{yang2021medmnist}
    Yang, J., Shi, R., Ni, B.:
    MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis.
    IEEE 18th International Symposium on Biomedical Imaging (ISBI), 191--195 (2021)

    \bibitem{kingma2014adam}
    Kingma, D.P., Ba, J.:
    Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980 (2014)

\end{thebibliography}

\end{document}
